<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.webp" color="#222">
  <meta name="google-site-verification" content="Cj9oDgXxdJe6MoA_lEUQ2rmOouwJeTm5uJNqjhOv8Ng">
  <meta name="msvalidate.01" content="E5D0AA8F5E012DFD9C5F3954DF8283B1">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,700,700italic%7CNoto+Serif+SC:300,300italic,400,400italic,700,700italic%7CSource+Code+Pro:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"shenxiaohai.me","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"default"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":"twikoo","storage":true,"lazyload":false,"nav":null,"activeClass":"twikoo"},"stickytabs":false,"motion":{"enable":false,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="论文 《Cyclical Learning Rates for Training Neural Networks》的笔记。论文描述了一种搜索最优学习率的方法，以及一种用于训练网络的学习设置的方法（周期性学习率）。">
<meta property="og:type" content="article">
<meta property="og:title" content="寻找最优学习率的方法以及周期性学习率更新策略(论文笔记)">
<meta property="og:url" content="https://shenxiaohai.me/lr_find/index.html">
<meta property="og:site_name" content="SHEN&#39;s DevNotes">
<meta property="og:description" content="论文 《Cyclical Learning Rates for Training Neural Networks》的笔记。论文描述了一种搜索最优学习率的方法，以及一种用于训练网络的学习设置的方法（周期性学习率）。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://res.cloudinary.com/dgchmgebr/image/upload/v1678509797/blog/gcwxysr34cn5qd3vghno.png">
<meta property="og:image" content="chttps://res.cloudinary.com/dgchmgebr/image/upload/v1678509797/blog/gcwxysr34cn5qd3vghno.png">
<meta property="og:image" content="https://res.cloudinary.com/dgchmgebr/image/upload/v1678509799/blog/yhijgekupl2itfeyjxsr.png">
<meta property="og:image" content="https://res.cloudinary.com/dgchmgebr/image/upload/v1678509799/blog/yjav3ilsfil6yjugl7ta.png">
<meta property="og:image" content="https://res.cloudinary.com/dgchmgebr/image/upload/v1678509800/blog/dkq4cxnlgxg3mwaahvmf.png">
<meta property="og:image" content="https://res.cloudinary.com/dgchmgebr/image/upload/v1678509800/blog/yihzdvk3uxsehozasrhn.png">
<meta property="og:image" content="https://res.cloudinary.com/dgchmgebr/image/upload/v1678509801/blog/vaiseewhrfkwxiexogzj.png">
<meta property="og:image" content="https://res.cloudinary.com/dgchmgebr/image/upload/v1678509801/blog/mqyfhmbr0a5wcff0md6k.png">
<meta property="og:image" content="https://res.cloudinary.com/dgchmgebr/image/upload/v1678509802/blog/febwi05ty0lcc7qofoac.webp">
<meta property="article:published_time" content="2019-02-26T19:56:54.000Z">
<meta property="article:modified_time" content="2023-03-11T05:03:25.874Z">
<meta property="article:author" content="xiaohai">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="论文笔记">
<meta property="article:tag" content="技巧">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://res.cloudinary.com/dgchmgebr/image/upload/v1678509797/blog/gcwxysr34cn5qd3vghno.png">


<link rel="canonical" href="https://shenxiaohai.me/lr_find/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://shenxiaohai.me/lr_find/","path":"lr_find/","title":"寻找最优学习率的方法以及周期性学习率更新策略(论文笔记)"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>寻找最优学习率的方法以及周期性学习率更新策略(论文笔记) | SHEN's DevNotes</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-YDT7F4NZP6"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"G-YDT7F4NZP6","only_pageview":false}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="SHEN's DevNotes" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">SHEN's DevNotes</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">学习笔记，编程技巧，效率工具</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">135</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">13</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">74</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Cyclical-Learning-Rates-for-Training-Neural-Networks"><span class="nav-text">Cyclical Learning Rates for Training Neural Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%80%E4%BC%98%E5%AD%A6%E4%B9%A0%E7%8E%87%EF%BC%9F%E5%AD%A6%E4%B9%A0%E7%8E%87%E7%9A%84%E6%9B%B4%E6%96%B0%E7%AD%96%E7%95%A5%EF%BC%9F"><span class="nav-text">最优学习率？学习率的更新策略？</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%80%E4%BC%98%E5%AD%A6%E4%B9%A0%E7%8E%87%EF%BC%9F"><span class="nav-text">最优学习率？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E7%9A%84%E6%9B%B4%E6%96%B0%E7%AD%96%E7%95%A5%EF%BC%9F"><span class="nav-text">学习率的更新策略？</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%80%E4%BC%98%E5%AD%A6%E4%B9%A0%E7%8E%87"><span class="nav-text">最优学习率</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%91%A8%E6%9C%9F%E6%80%A7%E5%AD%A6%E4%B9%A0%E7%8E%87%EF%BC%88Cyclical-Learning-Rates%EF%BC%89"><span class="nav-text">周期性学习率（Cyclical Learning Rates）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F"><span class="nav-text">计算公式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%91%A8%E6%9C%9F%E6%80%A7%E5%AD%A6%E4%B9%A0%E7%8E%87%E7%9A%84%E5%87%A0%E7%A7%8D%E5%BD%A2%E5%BC%8F"><span class="nav-text">周期性学习率的几种形式</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#triangular"><span class="nav-text">triangular</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#triangular2%EF%BC%88%E5%9B%BA%E5%AE%9A%E5%91%A8%E6%9C%9F%E8%A1%B0%E5%87%8F%EF%BC%89"><span class="nav-text">triangular2（固定周期衰减）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#exp-range%EF%BC%88%E6%8C%87%E6%95%B0%E5%91%A8%E6%9C%9F%E8%A1%B0%E5%87%8F%EF%BC%89"><span class="nav-text">exp_range（指数周期衰减）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%89%E6%8B%A9%E5%90%88%E7%90%86%E7%9A%84maximum%E5%92%8Cminimum%E8%BE%B9%E7%95%8C%E5%80%BC"><span class="nav-text">选择合理的maximum和minimum边界值</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83"><span class="nav-text">参考</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">xiaohai</p>
  <div class="site-description" itemprop="description">我在这个技术博客中分享学习笔记，提供实用技巧和工具资源。</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">74</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">135</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/veraposeidon" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;veraposeidon" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/HiYoake" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;HiYoake" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/big/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://shenxiaohai.me/lr_find/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="xiaohai">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SHEN's DevNotes">
      <meta itemprop="description" content="我在这个技术博客中分享学习笔记，提供实用技巧和工具资源。">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="寻找最优学习率的方法以及周期性学习率更新策略(论文笔记) | SHEN's DevNotes">
      <meta itemprop="description" content="论文 《Cyclical Learning Rates for Training Neural Networks》的笔记。论文描述了一种搜索最优学习率的方法，以及一种用于训练网络的学习设置的方法（周期性学习率）。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          寻找最优学习率的方法以及周期性学习率更新策略(论文笔记)
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-02-26 19:56:54" itemprop="dateCreated datePublished" datetime="2019-02-26T19:56:54Z">2019-02-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-03-11 05:03:25" itemprop="dateModified" datetime="2023-03-11T05:03:25Z">2023-03-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" itemprop="url" rel="index"><span itemprop="name">论文笔记</span></a>
        </span>
    </span>

  
</div>

            <div class="post-description">论文 《Cyclical Learning Rates for Training Neural Networks》的笔记。论文描述了一种搜索最优学习率的方法，以及一种用于训练网络的学习设置的方法（周期性学习率）。</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="Cyclical-Learning-Rates-for-Training-Neural-Networks"><a href="#Cyclical-Learning-Rates-for-Training-Neural-Networks" class="headerlink" title="Cyclical Learning Rates for Training Neural Networks"></a>Cyclical Learning Rates for Training Neural Networks</h1><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1506.01186">Cyclical Learning Rates for Training Neural Networks</a> 论文笔记。</p>
<p>学习Jeremy Howard的MOOC课程fast.ai2018和fast.ai2019的时候，印象最深的就是fast.ai库的训练速度之快。每次开场，Jeremy老师都要拿出这篇Cycical论文出来说道一波。从每个课件jupyter必备的函数<code>learn.lr_find()</code>，就能看出这篇论文占据了fast.ai一席之地。</p>
<p>一直都想用fast.ai用来做自己的项目，可是它到现在都只是能够训练CV里的分类网络（捎带一下分割网络），就是迟迟不来用在目标检测上的框架，手动DIY实在是太难了！！！</p>
<p>没办法，fast.ai之所以fast的思想还是需要掌握，今天把这篇论文给看了，写个笔记做个总结。结果网上已经出了一大堆根据这篇论文写的关于寻找最优学习率的博客，那就拿来主义，做份摘录好了。谢谢参考下的所有博客！</p>
<p><strong>这篇论文讲的是实操技巧，作者还没有挖掘现象背后的理论解释。</strong></p>
<h2 id="最优学习率？学习率的更新策略？"><a href="#最优学习率？学习率的更新策略？" class="headerlink" title="最优学习率？学习率的更新策略？"></a>最优学习率？学习率的更新策略？</h2><h3 id="最优学习率？"><a href="#最优学习率？" class="headerlink" title="最优学习率？"></a>最优学习率？</h3><p><img data-src="https://res.cloudinary.com/dgchmgebr/image/upload/v1678509797/blog/gcwxysr34cn5qd3vghno.png" alt="lr"></p>
<p><a target="_blank" rel="noopener" href="https://www.jeremyjordan.me/nn-learning-rate/">ImageCredit</a></p>
<p>开源的几个著名的深度学习CV工具包，Detectron、Maskrcnn_benchmark等，虽然提供参考的配置模板，但里面的学习率都是对应于Pascal VOC、COCO这些数据集，应用在自定义数据集上的可用性又低了一个档次。所以要自己选择一个合适的学习率。</p>
<p>比较笨的方法么就是靠不停地迭代实验进行测试的，就是试错嘛。试错的代价嘛，就是浪费时间啊。</p>
<p>学习率设置得低，自然稳扎稳打，训练可靠，但是收敛的速度很慢很慢；</p>
<p>学习率设置得高，训练速度就算上来了，最终还是容易从震荡到发散，优化等于没有意义。</p>
<p>所以有一个好的学习率，或者初始学习率还是蛮重要的。</p>
<h3 id="学习率的更新策略？"><a href="#学习率的更新策略？" class="headerlink" title="学习率的更新策略？"></a>学习率的更新策略？</h3><p>网络的整个学习过程不是一成不变的，刚开始Loss会有较大幅度的下降，等到训练一段时间以后，参数的梯度变小了，要想继续优化，也应当变小参数的更新幅度。所以都会在训练中随着训练慢慢降低学习率。</p>
<p>常见的一些学习率更新策略包括，步进式的（Step Decay），余弦式的（Cosine Decay）等，还有一些是自适应学速率比如RMSProp、AdaDelta、AdaSecant、ESGD等，这些呢，都是通过在训练过程中，通过权重、梯度等信息计算出来的。</p>
<p>这篇论文的作者发现并总结了一个新的学习率的更新策略，称之为Cyclical Learning Rates（周期性学习率），简称<strong>CLR</strong>，并且取得了不错的训练效果。</p>
<p><img data-src="chttps://res.cloudinary.com/dgchmgebr/image/upload/v1678509797/blog/gcwxysr34cn5qd3vghno.png" alt="CLR"></p>
<h2 id="最优学习率"><a href="#最优学习率" class="headerlink" title="最优学习率"></a>最优学习率</h2><p>整篇文章其实介绍了两个重要的点：</p>
<ul>
<li>周期性学习率的介绍和几个参数设置</li>
<li>周期性学习率的上界和下界的设置（最优学习率的查找）</li>
</ul>
<h3 id="周期性学习率（Cyclical-Learning-Rates）"><a href="#周期性学习率（Cyclical-Learning-Rates）" class="headerlink" title="周期性学习率（Cyclical Learning Rates）"></a>周期性学习率（Cyclical Learning Rates）</h3><p><img data-src="https://res.cloudinary.com/dgchmgebr/image/upload/v1678509799/blog/yhijgekupl2itfeyjxsr.png" alt="policy"></p>
<p>看图就很容易明白：</p>
<ol>
<li>周期性的；</li>
<li>学习率值在两个约束值（max_lr和min_lr）之间变化；</li>
<li>图示是三角形的更新规则，称之为Triangular learning rate policy.</li>
</ol>
<p>这种方式的优越性在哪里？</p>
<p>作者也提到了：最小化训练损失的难点不在于局部最小值（local minima），而在于鞍点（saddle points），因为鞍点这个区域具有小的梯度值，减慢了训练的过程。直观的解释就是周期性的学习率能够在上升学习率的时候较快的穿过这个区域。</p>
<h3 id="计算公式"><a href="#计算公式" class="headerlink" title="计算公式"></a>计算公式</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cycle = np.floor(<span class="number">1</span>+iterations/(<span class="number">2</span>*step_size))	<span class="comment"># 当前iteration 处于第几个cycle</span></span><br><span class="line">x = np.<span class="built_in">abs</span>(iterations/step_size - <span class="number">2</span>*cycle + <span class="number">1</span>)	<span class="comment"># 当前当前iteration 处于三角形上下沿的比例</span></span><br><span class="line">lr= base_lr + (max_lr-base_lr)*np.maximum(<span class="number">0</span>, (<span class="number">1</span>-x))*scale_fn(x)	<span class="comment"># 根据比例求学习率</span></span><br></pre></td></tr></table></figure>

<p>拿CIFAR-10来举例，共有50000张图像，设置BatchSize为100，则$epoch &#x3D; 50000&#x2F;100&#x3D;500 iterations$.</p>
<p>设置$step_size$为2-10倍的iterations。step_size表示cycle的一般。</p>
<h3 id="周期性学习率的几种形式"><a href="#周期性学习率的几种形式" class="headerlink" title="周期性学习率的几种形式"></a>周期性学习率的几种形式</h3><h4 id="triangular"><a href="#triangular" class="headerlink" title="triangular"></a>triangular</h4><p><img data-src="https://res.cloudinary.com/dgchmgebr/image/upload/v1678509799/blog/yjav3ilsfil6yjugl7ta.png" alt="triangularDiag"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/bckenstler/CLR">Image credit</a></p>
<p><strong>计算方式</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cycle = np.floor(<span class="number">1</span>+iterations/(<span class="number">2</span>*step_size))</span><br><span class="line">x = np.<span class="built_in">abs</span>(iterations/step_size - <span class="number">2</span>*cycle + <span class="number">1</span>)</span><br><span class="line">lr = base_lr + (max_lr-base_lr)*np.maximum(<span class="number">0</span>, (<span class="number">1</span>-x))</span><br></pre></td></tr></table></figure>

<h4 id="triangular2（固定周期衰减）"><a href="#triangular2（固定周期衰减）" class="headerlink" title="triangular2（固定周期衰减）"></a>triangular2（固定周期衰减）</h4><p>如图，最高点学习率每轮都减半。</p>
<p><img data-src="https://res.cloudinary.com/dgchmgebr/image/upload/v1678509800/blog/dkq4cxnlgxg3mwaahvmf.png" alt="triangular2Diag"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/bckenstler/CLR">Image credit</a></p>
<p><strong>计算方式</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cycle = np.floor(<span class="number">1</span>+iterations/(<span class="number">2</span>*step_size))</span><br><span class="line">x = np.<span class="built_in">abs</span>(iterations/step_size - <span class="number">2</span>*cycle + <span class="number">1</span>)</span><br><span class="line">lr = base_lr + (max_lr-base_lr)*np.maximum(<span class="number">0</span>, (<span class="number">1</span>-x))/<span class="built_in">float</span>(<span class="number">2</span>**(cycle-<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<h4 id="exp-range（指数周期衰减）"><a href="#exp-range（指数周期衰减）" class="headerlink" title="exp_range（指数周期衰减）"></a>exp_range（指数周期衰减）</h4><p>如图，最高点学习率每轮都乘以一个gamma因子。</p>
<p><img data-src="https://res.cloudinary.com/dgchmgebr/image/upload/v1678509800/blog/yihzdvk3uxsehozasrhn.png" alt="exp_rangeDiag"></p>
<p><a target="_blank" rel="noopener" href="https://github.com/bckenstler/CLR">Image credit</a></p>
<p><strong>计算方式</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cycle = np.floor(<span class="number">1</span>+iterations/(<span class="number">2</span>*step_size))</span><br><span class="line">x = np.<span class="built_in">abs</span>(iterations/step_size - <span class="number">2</span>*cycle + <span class="number">1</span>)</span><br><span class="line">lr= base_lr + (max_lr-base_lr)*np.maximum(<span class="number">0</span>, (<span class="number">1</span>-x))*gamma**(iterations)</span><br></pre></td></tr></table></figure>

<h3 id="选择合理的maximum和minimum边界值"><a href="#选择合理的maximum和minimum边界值" class="headerlink" title="选择合理的maximum和minimum边界值"></a>选择合理的maximum和minimum边界值</h3><p>就是如何选择最优的学习率的问题。</p>
<p>其实作者提出的办法还是很简单的，就是不断地迭代，每次迭代使用不同的学习率lr，记录下每次迭代的损失值loss，通过绘制（loss-lr）曲线来观察选择合适的学习率。</p>
<p>如图所示，从一个较低的或者说极低的学习率开始训练网络，并且在每个迭代中慢慢提高学习率（指数式）：</p>
<p><img data-src="https://res.cloudinary.com/dgchmgebr/image/upload/v1678509801/blog/vaiseewhrfkwxiexogzj.png" alt="学习率"></p>
<p><a target="_blank" rel="noopener" href="https://www.jiqizhixin.com/articles/2017-11-17-2">Image credit</a></p>
<p>每次迭代都记录下当前的学习率和训练loss，结束后根据这俩数据绘制loss-learningrate的曲线，如图：</p>
<p><img data-src="https://res.cloudinary.com/dgchmgebr/image/upload/v1678509801/blog/mqyfhmbr0a5wcff0md6k.png" alt="损失值"></p>
<p><a target="_blank" rel="noopener" href="https://www.jiqizhixin.com/articles/2017-11-17-2">Image credit</a></p>
<p>接下来要确定最优的学习率：</p>
<ul>
<li><p>论文的作者记录的是accuracy-lr曲线，然后他是从accuracy开始上升到开始下降这段区间，用这个区间来确定min_lr和max_lr。<br>按照图示就是绿色区间的首尾横坐标。</p>
<p><img data-src="https://res.cloudinary.com/dgchmgebr/image/upload/v1678509802/blog/febwi05ty0lcc7qofoac.webp" alt="area"></p>
</li>
<li><p>如果要用最优学习率的话，就取max_lr的1&#x2F;3或1&#x2F;4，用fast.ai Jeremy 的话讲就是loss曲线下降最快的地方。</p>
</li>
</ul>
<p>附上一份PyTorch实现寻找学习率的代码，可以根据自己情况修改：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">find_lr</span>(<span class="params">init_value = <span class="number">1e-8</span>, final_value=<span class="number">10.</span>, beta = <span class="number">0.98</span></span>):</span><br><span class="line">    num = <span class="built_in">len</span>(trn_loader)-<span class="number">1</span></span><br><span class="line">    mult = (final_value / init_value) ** (<span class="number">1</span>/num)</span><br><span class="line">    lr = init_value</span><br><span class="line">    optimizer.param_groups[<span class="number">0</span>][<span class="string">&#x27;lr&#x27;</span>] = lr</span><br><span class="line">    avg_loss = <span class="number">0.</span></span><br><span class="line">    best_loss = <span class="number">0.</span></span><br><span class="line">    batch_num = <span class="number">0</span></span><br><span class="line">    losses = []</span><br><span class="line">    log_lrs = []</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> trn_loader:</span><br><span class="line">        batch_num += <span class="number">1</span></span><br><span class="line">        <span class="comment">#As before, get the loss for this mini-batch of inputs/outputs</span></span><br><span class="line">        inputs,labels = data</span><br><span class="line">        inputs, labels = Variable(inputs), Variable(labels)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        <span class="comment">#Compute the smoothed loss</span></span><br><span class="line">        avg_loss = beta * avg_loss + (<span class="number">1</span>-beta) *loss.data[<span class="number">0</span>]</span><br><span class="line">        smoothed_loss = avg_loss / (<span class="number">1</span> - beta**batch_num)</span><br><span class="line">        <span class="comment">#Stop if the loss is exploding</span></span><br><span class="line">        <span class="keyword">if</span> batch_num &gt; <span class="number">1</span> <span class="keyword">and</span> smoothed_loss &gt; <span class="number">4</span> * best_loss:</span><br><span class="line">            <span class="keyword">return</span> log_lrs, losses</span><br><span class="line">        <span class="comment">#Record the best loss</span></span><br><span class="line">        <span class="keyword">if</span> smoothed_loss &lt; best_loss <span class="keyword">or</span> batch_num==<span class="number">1</span>:</span><br><span class="line">            best_loss = smoothed_loss</span><br><span class="line">        <span class="comment">#Store the values</span></span><br><span class="line">        losses.append(smoothed_loss)</span><br><span class="line">        log_lrs.append(math.log10(lr))</span><br><span class="line">        <span class="comment">#Do the SGD step</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="comment">#Update the lr for the next step</span></span><br><span class="line">        lr *= mult</span><br><span class="line">        optimizer.param_groups[<span class="number">0</span>][<span class="string">&#x27;lr&#x27;</span>] = lr</span><br><span class="line">    <span class="keyword">return</span> log_lrs, losses</span><br></pre></td></tr></table></figure>

<p>代码来自博客<a target="_blank" rel="noopener" href="https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html">How Do You Find A Good Learning Rate</a>。</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.jeremyjordan.me/nn-learning-rate/">Setting the learning rate of your neural network.</a></li>
<li><a target="_blank" rel="noopener" href="https://juejin.im/post/5a9cc6f66fb9a028be357b24">机器学习算法如何调参？这里有一份神经网络学习速率设置指南</a></li>
<li><a target="_blank" rel="noopener" href="https://www.leiphone.com/news/201711/apwZec5F44oVRc2T.html">如何找到最优学习率？</a></li>
<li><a target="_blank" rel="noopener" href="https://www.jiqizhixin.com/articles/2017-11-17-2">如何估算深度神经网络的最优学习率</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/ranjiewen/p/9326201.html">sgd学习率选择问题</a></li>
<li><a target="_blank" rel="noopener" href="https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html">How Do You Find A Good Learning Rate</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/bckenstler/CLR">GitRepo 实现参考</a></li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>xiaohai
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://shenxiaohai.me/lr_find/" title="寻找最优学习率的方法以及周期性学习率更新策略(论文笔记)">https://shenxiaohai.me/lr_find/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 深度学习</a>
              <a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" rel="tag"><i class="fa fa-tag"></i> 论文笔记</a>
              <a href="/tags/%E6%8A%80%E5%B7%A7/" rel="tag"><i class="fa fa-tag"></i> 技巧</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/bag-freebies/" rel="prev" title="目标检测训练技巧集锦（论文笔记）">
                  <i class="fa fa-chevron-left"></i> 目标检测训练技巧集锦（论文笔记）
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/codecraft-2019/" rel="next" title="华为软挑代码开源及思路介绍">
                  华为软挑代码开源及思路介绍 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments"><div id="twikoo-comments"></div></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2018 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-laptop"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">xiaohai</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/veraposeidon" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.8/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":3000,"priority":true,"url":"https://shenxiaohai.me/lr_find/"}</script>
  <script src="/js/third-party/quicklink.js"></script>
<script class="next-config" data-name="twikoo" type="application/json">{"enable":true,"visitor":false,"envId":"https://twikoo-comment.shenxiaohai.me/","el":"#twikoo-comments"}</script>
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.twikoo.el)
    .then(() => NexT.utils.getScript(
      CONFIG.twikoo.jsUrl || 'https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js',
      { condition: window.twikoo }
    ))
    .then(() => {
      twikoo.init(CONFIG.twikoo);
    });
});
</script>
<style>
.post-block, .comments {
  overflow: visible;
}
.tk-owo-emotion {
  display: inline-block;
}
</style>

</body>
</html>
