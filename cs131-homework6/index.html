<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.webp" color="#222">
  <meta name="google-site-verification" content="Cj9oDgXxdJe6MoA_lEUQ2rmOouwJeTm5uJNqjhOv8Ng">
  <meta name="msvalidate.01" content="E5D0AA8F5E012DFD9C5F3954DF8283B1">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,700,700italic%7CNoto+Serif+SC:300,300italic,400,400italic,700,700italic%7CSource+Code+Pro:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"shenxiaohai.me","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.15.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"default"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":true,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":"twikoo","storage":true,"lazyload":false,"nav":null,"activeClass":"twikoo"},"stickytabs":false,"motion":{"enable":false,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="CS131 Computer Vision： Foundations and Applications，Homework6：Recognition-Classification。 实现分类算法。使用SVD进行图像压缩，使用KNN进行图像识别（分类），使用PCA（主成分分析）和LDA（线性判别分析）对数据进行降维。">
<meta property="og:type" content="article">
<meta property="og:title" content="CS131,Homewrok6,Recognition-Classification">
<meta property="og:url" content="https://shenxiaohai.me/cs131-homework6/index.html">
<meta property="og:site_name" content="SHEN&#39;s DevNotes">
<meta property="og:description" content="CS131 Computer Vision： Foundations and Applications，Homework6：Recognition-Classification。 实现分类算法。使用SVD进行图像压缩，使用KNN进行图像识别（分类），使用PCA（主成分分析）和LDA（线性判别分析）对数据进行降维。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511085/blog/qhplwa5uxtbhx5wgjx7b.png">
<meta property="og:image" content="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511085/blog/hirobh4zx7mer88pefjy.png">
<meta property="og:image" content="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511086/blog/dblvtpnt7lilqku0ea3e.png">
<meta property="og:image" content="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511087/blog/wordwcgtmqh2yz6s5udl.png">
<meta property="og:image" content="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511087/blog/oewkeaxbvekw7qgf5iij.png">
<meta property="og:image" content="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511088/blog/gadwvning6nywtkmx4zs.png">
<meta property="og:image" content="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511088/blog/bxtpp1m2cgsxqrqwdboe.png">
<meta property="og:image" content="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511089/blog/wlxmgfikbtn6sp6mumdj.png">
<meta property="og:image" content="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511090/blog/tjq8g5fgcrf2scve7mdg.png">
<meta property="og:image" content="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511090/blog/uyaie9xmwg228mq6mip1.png">
<meta property="og:image" content="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511091/blog/ucsqof75wk9yhwrruod7.png">
<meta property="og:image" content="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511091/blog/cjq1hg59vcc5ik9owzsd.png">
<meta property="og:image" content="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511092/blog/wwwgwlp65dxempx2kflv.jpg">
<meta property="og:image" content="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511092/blog/fjkefvwgvpny1i24r6qk.png">
<meta property="og:image" content="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511093/blog/qxiyfyuferuubuvlioog.png">
<meta property="og:image" content="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511094/blog/vepeiprwymewx0lk7tmx.png">
<meta property="og:image" content="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511094/blog/tnfpo9wwgt8hqivsnal1.png">
<meta property="article:published_time" content="2018-09-19T22:14:19.000Z">
<meta property="article:modified_time" content="2023-03-11T05:04:55.306Z">
<meta property="article:author" content="xiaohai">
<meta property="article:tag" content="计算机视觉">
<meta property="article:tag" content="CS131">
<meta property="article:tag" content="PCA">
<meta property="article:tag" content="LDA">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511085/blog/qhplwa5uxtbhx5wgjx7b.png">


<link rel="canonical" href="https://shenxiaohai.me/cs131-homework6/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://shenxiaohai.me/cs131-homework6/","path":"cs131-homework6/","title":"CS131,Homewrok6,Recognition-Classification"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>CS131,Homewrok6,Recognition-Classification | SHEN's DevNotes</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-YDT7F4NZP6"></script>
  <script class="next-config" data-name="google_analytics" type="application/json">{"tracking_id":"G-YDT7F4NZP6","only_pageview":false}</script>
  <script src="/js/third-party/analytics/google-analytics.js"></script>








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<link rel="alternate" href="/atom.xml" title="SHEN's DevNotes" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">SHEN's DevNotes</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">学习笔记，编程技巧，效率工具</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">131</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">18</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">74</span></a></li><li class="menu-item menu-item-碎碎念-|-memos"><a href="https://memos.shenxiaohai.me/" rel="section" target="_blank"><i class="fa fa-file-pen fa-fw"></i>碎碎念 | Memos</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Homework-6"><span class="nav-text">Homework 6</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Part-1-Image-Compression-15-points"><span class="nav-text">Part 1 - Image Compression (15 points)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Face-Dataset"><span class="nav-text">Face Dataset</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Part-2-k-Nearest-Neighbor-30-points"><span class="nav-text">Part 2 - k-Nearest Neighbor (30 points)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Cross-Validation"><span class="nav-text">Cross-Validation</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Part-3-PCA-30-points"><span class="nav-text">Part 3: PCA (30 points)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Introduction"><span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#A-Summary-of-the-PCA-Approach"><span class="nav-text">A Summary of the PCA Approach</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-Eigendecomposition"><span class="nav-text">3.1 - Eigendecomposition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-Singular-Value-Decomposition"><span class="nav-text">3.2 - Singular Value Decomposition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-Dimensionality-Reduction"><span class="nav-text">3.3 - Dimensionality Reduction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-Visualizing-Eigenfaces"><span class="nav-text">3.4 - Visualizing Eigenfaces</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Written-Question-1-5-points"><span class="nav-text">Written Question 1 (5 points)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-Reconstruction-error-and-captured-variance"><span class="nav-text">3.5 - Reconstruction error and captured variance</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-6-kNN-with-PCA"><span class="nav-text">3.6 - kNN with PCA</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Part-4-Fisherface-Linear-Discriminant-Analysis-25-points"><span class="nav-text">Part 4 - Fisherface: Linear Discriminant Analysis (25 points)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-Dimensionality-Reduction-via-PCA"><span class="nav-text">4.1 - Dimensionality Reduction via PCA</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-Scatter-matrices"><span class="nav-text">4.2 - Scatter matrices</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-Solving-generalized-Eigenvalue-problem"><span class="nav-text">4.3 - Solving generalized Eigenvalue problem</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-kNN-with-LDA"><span class="nav-text">4.4 - kNN with LDA</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E6%A1%A3%E6%A1%88"><span class="nav-text">代码档案</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">xiaohai</p>
  <div class="site-description" itemprop="description">我在这个技术博客中分享学习笔记，提供实用技巧和工具资源。</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">74</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">131</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/veraposeidon" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;veraposeidon" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://twitter.com/HiYoake" title="Twitter → https:&#x2F;&#x2F;twitter.com&#x2F;HiYoake" rel="noopener me" target="_blank"><i class="fab fa-twitter fa-fw"></i>Twitter</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/big/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://shenxiaohai.me/cs131-homework6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="xiaohai">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="SHEN's DevNotes">
      <meta itemprop="description" content="我在这个技术博客中分享学习笔记，提供实用技巧和工具资源。">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="CS131,Homewrok6,Recognition-Classification | SHEN's DevNotes">
      <meta itemprop="description" content="CS131 Computer Vision： Foundations and Applications，Homework6：Recognition-Classification。<br> 实现分类算法。<br>使用SVD进行图像压缩，<br>使用KNN进行图像识别（分类），<br>使用PCA（主成分分析）和LDA（线性判别分析）对数据进行降维。">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          CS131,Homewrok6,Recognition-Classification
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2018-09-19 22:14:19" itemprop="dateCreated datePublished" datetime="2018-09-19T22:14:19Z">2018-09-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-03-11 05:04:55" itemprop="dateModified" datetime="2023-03-11T05:04:55Z">2023-03-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" itemprop="url" rel="index"><span itemprop="name">计算机视觉</span></a>
        </span>
    </span>

  
</div>

            <div class="post-description">CS131 Computer Vision： Foundations and Applications，Homework6：Recognition-Classification。<br> 实现分类算法。<br>使用SVD进行图像压缩，<br>使用KNN进行图像识别（分类），<br>使用PCA（主成分分析）和LDA（线性判别分析）对数据进行降维。</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="Homework-6"><a href="#Homework-6" class="headerlink" title="Homework 6"></a>Homework 6</h1><p><em>This notebook includes both coding and written questions. Please hand in this notebook file with all the outputs as a pdf on gradescope for “HW6 pdf”. Upload the three files of code (<code>compression.py</code>, <code>k_nearest_neighbor.py</code> and <code>features.py</code>) on gradescope for “HW6 code”.</em></p>
<p>This assignment covers:</p>
<ul>
<li>image compression using SVD</li>
<li>kNN methods for image recognition.</li>
<li>PCA and LDA to improve kNN</li>
</ul>
<p>这份任务包括了：</p>
<ol>
<li>使用SVD进行图像压缩</li>
<li>使用kNN方法进行图像识别</li>
<li>使用PCA和LDA提升kNN性能</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Setup</span></span><br><span class="line"><span class="keyword">from</span> time <span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> rc</span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">plt.rcParams[<span class="string">&#x27;figure.figsize&#x27;</span>] = (<span class="number">15.0</span>, <span class="number">12.0</span>) <span class="comment"># set default size of plots</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;image.interpolation&#x27;</span>] = <span class="string">&#x27;nearest&#x27;</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;image.cmap&#x27;</span>] = <span class="string">&#x27;gray&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># for auto-reloading external modules</span></span><br><span class="line">%load_ext autoreload</span><br><span class="line">%autoreload <span class="number">2</span></span><br></pre></td></tr></table></figure>

<pre><code>The autoreload extension is already loaded. To reload it, use:
  %reload_ext autoreload
</code></pre>
<h2 id="Part-1-Image-Compression-15-points"><a href="#Part-1-Image-Compression-15-points" class="headerlink" title="Part 1 - Image Compression (15 points)"></a>Part 1 - Image Compression (15 points)</h2><p>Image compression is used to reduce the cost of storage and transmission of images (or videos).<br>One lossy compression method is to apply Singular Value Decomposition (SVD) to an image, and only keep the top n singular values.</p>
<p>此处虽然称之为图像压缩，实际上就是数据处理里的降维的步骤。</p>
<p>有关SVD（奇异值分解），可以看这篇博客，博客都是我随便找的，看懂原理即可。</p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/19/svd-and-applications.html">机器学习中的数学(5)-强大的矩阵奇异值分解(SVD)及其应用</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">image = io.imread(<span class="string">&#x27;pitbull.jpg&#x27;</span>, as_grey=<span class="literal">True</span>)</span><br><span class="line">plt.imshow(image)</span><br><span class="line">plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img data-src="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511085/blog/qhplwa5uxtbhx5wgjx7b.png" alt="png"></p>
<p>Let’s implement image compression using SVD.<br>We first compute the SVD of the image, and as seen in class we keep the <code>n</code> largest singular values and singular vectors to reconstruct the image.</p>
<p>Implement function <code>compress_image</code> in <code>compression.py</code>.</p>
<p>在compress_image函数中实现奇异值分解，并保留前N个较大的奇异值和对应的特征向量。</p>
<p>题目中的压缩率在我理解就是压缩后有效数据与压缩之前图像数据的比值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> compression <span class="keyword">import</span> compress_image</span><br><span class="line"></span><br><span class="line">compressed_image, compressed_size = compress_image(image, <span class="number">100</span>)</span><br><span class="line">compression_ratio = compressed_size / image.size</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Original image shape:&#x27;</span>, image.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Compressed size: %d&#x27;</span> % compressed_size)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Compression ratio: %.3f&#x27;</span> % compression_ratio)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> compressed_size == <span class="number">298500</span></span><br></pre></td></tr></table></figure>

<pre><code>Original image shape: (1704, 1280)
Compressed size: 298500
Compression ratio: 0.137
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Number of singular values to keep</span></span><br><span class="line">n_values = [<span class="number">10</span>, <span class="number">50</span>, <span class="number">100</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> n_values:</span><br><span class="line">    <span class="comment"># Compress the image using `n` singular values</span></span><br><span class="line">    compressed_image, compressed_size = compress_image(image, n)</span><br><span class="line">    </span><br><span class="line">    compression_ratio = compressed_size / image.size</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Data size (original): %d&quot;</span> % (image.size))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Data size (compressed): %d&quot;</span> % compressed_size)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Compression ratio: %f&quot;</span> % (compression_ratio))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    plt.imshow(compressed_image, cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    title = <span class="string">&quot;n = %s&quot;</span> % n</span><br><span class="line">    plt.title(title)</span><br><span class="line">    plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>Data size (original): 2181120
Data size (compressed): 29850
Compression ratio: 0.013686
</code></pre>
<p><img data-src="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511085/blog/hirobh4zx7mer88pefjy.png" alt="png"></p>
<pre><code>Data size (original): 2181120
Data size (compressed): 149250
Compression ratio: 0.068428
</code></pre>
<p><img data-src="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511086/blog/dblvtpnt7lilqku0ea3e.png" alt="png"></p>
<pre><code>Data size (original): 2181120
Data size (compressed): 298500
Compression ratio: 0.136856
</code></pre>
<p><img data-src="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511087/blog/wordwcgtmqh2yz6s5udl.png" alt="png"></p>
<p>实现了用SVD实现了图像的压缩，同时可以发现：n&#x3D;100时，压缩率0.13时，图像质量就已经很接近原图了。</p>
<p>这样看来，SVD的确用来做降维是很合适的。毕竟能够大大提高运行速度。</p>
<h2 id="Face-Dataset"><a href="#Face-Dataset" class="headerlink" title="Face Dataset"></a>Face Dataset</h2><p>We will use a dataset of faces of celebrities. Download the dataset using the following command:</p>
<pre><code>sh get_dataset.sh
</code></pre>
<p>The face dataset for CS131 assignment.<br>The directory containing the dataset has the following structure:</p>
<pre><code>faces/
    train/
        angelina jolie/
        anne hathaway/
        ...
    test/
        angelina jolie/
        anne hathaway/
        ...
</code></pre>
<p>Each class has 50 training images and 10 testing images.</p>
<p>命令行执行<code>sh get_dataset.sh</code>就可以下载了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载数据集</span></span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> load_dataset</span><br><span class="line"></span><br><span class="line">X_train, y_train, classes_train = load_dataset(<span class="string">&#x27;faces&#x27;</span>, train=<span class="literal">True</span>, as_grey=<span class="literal">True</span>)</span><br><span class="line">X_test, y_test, classes_test = load_dataset(<span class="string">&#x27;faces&#x27;</span>, train=<span class="literal">False</span>, as_grey=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> classes_train == classes_test</span><br><span class="line">classes = classes_train</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Class names:&#x27;</span>, classes)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Training data shape:&#x27;</span>, X_train.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Training labels shape: &#x27;</span>, y_train.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Test data shape:&#x27;</span>, X_test.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Test labels shape: &#x27;</span>, y_test.shape)</span><br></pre></td></tr></table></figure>

<pre><code>Class names: [&#39;angelina jolie&#39;, &#39;anne hathaway&#39;, &#39;barack obama&#39;, &#39;brad pitt&#39;, &#39;cristiano ronaldo&#39;, &#39;emma watson&#39;, &#39;george clooney&#39;, &#39;hillary clinton&#39;, &#39;jennifer aniston&#39;, &#39;johnny depp&#39;, &#39;justin timberlake&#39;, &#39;leonardo dicaprio&#39;, &#39;natalie portman&#39;, &#39;nicole kidman&#39;, &#39;scarlett johansson&#39;, &#39;tom cruise&#39;]
Training data shape: (800, 64, 64)
Training labels shape:  (800,)
Test data shape: (160, 64, 64)
Test labels shape:  (160,)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可视化数据集中的一些样本</span></span><br><span class="line"><span class="comment"># 从每一类中显示一些样本</span></span><br><span class="line">num_classes = <span class="built_in">len</span>(classes)</span><br><span class="line">samples_per_class = <span class="number">10</span></span><br><span class="line"><span class="keyword">for</span> y, cls <span class="keyword">in</span> <span class="built_in">enumerate</span>(classes):</span><br><span class="line">    idxs = np.flatnonzero(y_train == y)</span><br><span class="line">    idxs = np.random.choice(idxs, samples_per_class, replace=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">for</span> i, idx <span class="keyword">in</span> <span class="built_in">enumerate</span>(idxs):</span><br><span class="line">        plt_idx = i * num_classes + y + <span class="number">1</span></span><br><span class="line">        plt.subplot(samples_per_class, num_classes, plt_idx)</span><br><span class="line">        plt.imshow(X_train[idx])</span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            plt.title(y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img data-src="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511087/blog/oewkeaxbvekw7qgf5iij.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将图像数据展开为行，即每个样本为一行</span></span><br><span class="line"><span class="comment"># 这样每个样本我们就有了4096个维度的特征</span></span><br><span class="line">X_train = np.reshape(X_train, (X_train.shape[<span class="number">0</span>], -<span class="number">1</span>))</span><br><span class="line">X_test = np.reshape(X_test, (X_test.shape[<span class="number">0</span>], -<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Training data shape:&quot;</span>, X_train.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Test data shape:&quot;</span>, X_test.shape)</span><br></pre></td></tr></table></figure>

<pre><code>Training data shape: (800, 4096)
Test data shape: (160, 4096)
</code></pre>
<hr>
<h2 id="Part-2-k-Nearest-Neighbor-30-points"><a href="#Part-2-k-Nearest-Neighbor-30-points" class="headerlink" title="Part 2 - k-Nearest Neighbor (30 points)"></a>Part 2 - k-Nearest Neighbor (30 points)</h2><p>We’re now going to try to classify the test images using the k-nearest neighbors algorithm on the <strong>raw features of the images</strong> (i.e. the pixel values themselves). We will see later how we can use kNN on better features.</p>
<p>Here are the steps that we will follow:</p>
<ol>
<li>We compute the L2 distances between every element of X_test and every element of X_train in <code>compute_distances</code>.</li>
<li>We split the dataset into 5 folds for cross-validation in <code>split_folds</code>.</li>
<li>For each fold, and for different values of <code>k</code>, we predict the labels and measure accuracy.</li>
<li>Using the best <code>k</code> found through cross-validation, we measure accuracy on the test set.</li>
</ol>
<blockquote>
<p><strong>博客推荐</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/ybjourney/p/4702562.html">机器学习（一）——K-近邻（KNN）算法</a></p>
</blockquote>
<p>kNN（k最近邻算法）的步骤如下：</p>
<ol>
<li>计算测试数据X_test中每个样本与训练数据X_train中每个样本之间的距离，距离计算方式为L2距离。</li>
<li>将数据集分割为5Folder（5折）</li>
<li>对每个子集和每个不同的参数k，预测结果并计算准确率</li>
<li>通过交叉验证找到最佳的参数k，并在测试集上评估准确率</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> k_nearest_neighbor <span class="keyword">import</span> compute_distances</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 1: compute the distances between all features from X_train and from X_test</span></span><br><span class="line"><span class="comment"># 第一步：计算训练集和测试集上所有特征的距离（L2），调用cdist函数即可</span></span><br><span class="line">dists = compute_distances(X_test, X_train)</span><br><span class="line"><span class="keyword">assert</span> dists.shape == (<span class="number">160</span>, <span class="number">800</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;dists shape:&quot;</span>, dists.shape)</span><br></pre></td></tr></table></figure>

<pre><code>dists shape: (160, 800)
</code></pre>
<p>预测结果分两步：</p>
<ol>
<li>找到预测点的最近的k个点</li>
<li>判断k个点中频率最高的train_label</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> k_nearest_neighbor <span class="keyword">import</span> predict_labels</span><br><span class="line"></span><br><span class="line"><span class="comment"># We use k = 1 (which corresponds to only taking the nearest neighbor to decide)</span></span><br><span class="line">y_test_pred = predict_labels(dists, y_train, k=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute and print the fraction of correctly predicted examples</span></span><br><span class="line">num_test = y_test.shape[<span class="number">0</span>]</span><br><span class="line">num_correct = np.<span class="built_in">sum</span>(y_test_pred == y_test)</span><br><span class="line">accuracy = <span class="built_in">float</span>(num_correct) / num_test</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Got %d / %d correct =&gt; accuracy: %f&#x27;</span> % (num_correct, num_test, accuracy))</span><br></pre></td></tr></table></figure>

<pre><code>Got 38 / 160 correct =&gt; accuracy: 0.237500
</code></pre>
<h3 id="Cross-Validation"><a href="#Cross-Validation" class="headerlink" title="Cross-Validation"></a>Cross-Validation</h3><p>We don’t know the best value for our parameter <code>k</code>.<br>There is no theory on how to choose an optimal <code>k</code>, and the way to choose it is through cross-validation.</p>
<p>We <strong>cannot</strong> compute any metric on the test set to choose the best <code>k</code>, because we want our final test accuracy to reflect a real use case. This real use case would be a setting where we have new examples come and we classify them on the go. There is no way to check the accuracy beforehand on that set of test examples to determine <code>k</code>.</p>
<p>Cross-validation will make use split the data into different fold (5 here).<br>For each fold, if we have a total of 5 folds we will have:</p>
<ul>
<li>80% of the data as training data</li>
<li>20% of the data as validation data</li>
</ul>
<p>We will compute the accuracy on the validation accuracy for each fold, and use the mean of these 5 accuracies to determine the best parameter <code>k</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> k_nearest_neighbor <span class="keyword">import</span> split_folds</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: split the data into 5 folds to perform cross-validation.</span></span><br><span class="line"><span class="comment"># 步骤二：分割数据为5折，执行交叉验证</span></span><br><span class="line">num_folds = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">X_trains, y_trains, X_vals, y_vals = split_folds(X_train, y_train, num_folds)</span><br><span class="line"></span><br><span class="line"><span class="keyword">assert</span> X_trains.shape == (<span class="number">5</span>, <span class="number">640</span>, <span class="number">4096</span>)</span><br><span class="line"><span class="keyword">assert</span> y_trains.shape == (<span class="number">5</span>, <span class="number">640</span>)</span><br><span class="line"><span class="keyword">assert</span> X_vals.shape == (<span class="number">5</span>, <span class="number">160</span>, <span class="number">4096</span>)</span><br><span class="line"><span class="keyword">assert</span> y_vals.shape == (<span class="number">5</span>, <span class="number">160</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Step 3: Measure the mean accuracy for each value of `k`</span></span><br><span class="line"><span class="comment"># 步骤三：计算每个参数k对应的平均准确率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># List of k to choose from</span></span><br><span class="line">k_choices = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">5</span>, <span class="number">101</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Dictionnary mapping k values to accuracies</span></span><br><span class="line"><span class="comment"># For each k value, we will have `num_folds` accuracies to compute</span></span><br><span class="line"><span class="comment"># k_to_accuracies[1] will be for instance [0.22, 0.23, 0.19, 0.25, 0.20] for 5 folds</span></span><br><span class="line">k_to_accuracies = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> k_choices:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Running for k=%d&quot;</span> % k)</span><br><span class="line">    accuracies = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_folds):</span><br><span class="line">        <span class="comment"># Make predictions</span></span><br><span class="line">        fold_dists = compute_distances(X_vals[i], X_trains[i])</span><br><span class="line">        y_pred = predict_labels(fold_dists, y_trains[i], k)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute and print the fraction of correctly predicted examples</span></span><br><span class="line">        num_correct = np.<span class="built_in">sum</span>(y_pred == y_vals[i])</span><br><span class="line">        accuracy = <span class="built_in">float</span>(num_correct) / <span class="built_in">len</span>(y_vals[i])</span><br><span class="line">        accuracies.append(accuracy)</span><br><span class="line">        </span><br><span class="line">    k_to_accuracies[k] = accuracies</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<pre><code>Running for k=5
Running for k=10
Running for k=15
Running for k=20
Running for k=25
Running for k=30
Running for k=35
Running for k=40
Running for k=45
Running for k=50
Running for k=55
Running for k=60
Running for k=65
Running for k=70
Running for k=75
Running for k=80
Running for k=85
Running for k=90
Running for k=95
Running for k=100
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plot the raw observations</span></span><br><span class="line"><span class="comment"># 绘图进行观察</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> k_choices:</span><br><span class="line">    accuracies = k_to_accuracies[k]</span><br><span class="line">    plt.scatter([k] * <span class="built_in">len</span>(accuracies), accuracies)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot the trend line with error bars that correspond to standard deviation</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line">accuracies_mean = np.array([np.mean(v) <span class="keyword">for</span> k,v <span class="keyword">in</span> <span class="built_in">sorted</span>(k_to_accuracies.items())])</span><br><span class="line">accuracies_std = np.array([np.std(v) <span class="keyword">for</span> k,v <span class="keyword">in</span> <span class="built_in">sorted</span>(k_to_accuracies.items())])</span><br><span class="line">plt.errorbar(k_choices, accuracies_mean, yerr=accuracies_std)</span><br><span class="line">plt.title(<span class="string">&#x27;Cross-validation on k&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Cross-validation accuracy&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img data-src="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511088/blog/gadwvning6nywtkmx4zs.png" alt="png"></p>
<p>从这幅图看，前30之内有两个峰值就达到了最高的准确率和较好的标准差。</p>
<p>可以进一步细化step进行测试。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Step 3: Measure the mean accuracy for each value of `k`</span></span><br><span class="line"><span class="comment"># 步骤三：计算每个参数k对应的平均准确率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># List of k to choose from</span></span><br><span class="line">k_choices = <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">31</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Dictionnary mapping k values to accuracies</span></span><br><span class="line"><span class="comment"># For each k value, we will have `num_folds` accuracies to compute</span></span><br><span class="line"><span class="comment"># k_to_accuracies[1] will be for instance [0.22, 0.23, 0.19, 0.25, 0.20] for 5 folds</span></span><br><span class="line">k_to_accuracies = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> k_choices:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Running for k=%d&quot;</span> % k)</span><br><span class="line">    accuracies = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_folds):</span><br><span class="line">        <span class="comment"># Make predictions</span></span><br><span class="line">        fold_dists = compute_distances(X_vals[i], X_trains[i])</span><br><span class="line">        y_pred = predict_labels(fold_dists, y_trains[i], k)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute and print the fraction of correctly predicted examples</span></span><br><span class="line">        num_correct = np.<span class="built_in">sum</span>(y_pred == y_vals[i])</span><br><span class="line">        accuracy = <span class="built_in">float</span>(num_correct) / <span class="built_in">len</span>(y_vals[i])</span><br><span class="line">        accuracies.append(accuracy)</span><br><span class="line">        </span><br><span class="line">    k_to_accuracies[k] = accuracies</span><br></pre></td></tr></table></figure>

<pre><code>Running for k=1
Running for k=2
Running for k=3
Running for k=4
Running for k=5
Running for k=6
Running for k=7
Running for k=8
Running for k=9
Running for k=10
Running for k=11
Running for k=12
Running for k=13
Running for k=14
Running for k=15
Running for k=16
Running for k=17
Running for k=18
Running for k=19
Running for k=20
Running for k=21
Running for k=22
Running for k=23
Running for k=24
Running for k=25
Running for k=26
Running for k=27
Running for k=28
Running for k=29
Running for k=30
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plot the raw observations</span></span><br><span class="line"><span class="comment"># 绘图进行观察</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> k_choices:</span><br><span class="line">    accuracies = k_to_accuracies[k]</span><br><span class="line">    plt.scatter([k] * <span class="built_in">len</span>(accuracies), accuracies)</span><br><span class="line"></span><br><span class="line"><span class="comment"># plot the trend line with error bars that correspond to standard deviation</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line">accuracies_mean = np.array([np.mean(v) <span class="keyword">for</span> k,v <span class="keyword">in</span> <span class="built_in">sorted</span>(k_to_accuracies.items())])</span><br><span class="line">accuracies_std = np.array([np.std(v) <span class="keyword">for</span> k,v <span class="keyword">in</span> <span class="built_in">sorted</span>(k_to_accuracies.items())])</span><br><span class="line">plt.errorbar(k_choices, accuracies_mean, yerr=accuracies_std)</span><br><span class="line">plt.title(<span class="string">&#x27;Cross-validation on k&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;k&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Cross-validation accuracy&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img data-src="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511088/blog/bxtpp1m2cgsxqrqwdboe.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Based on the cross-validation results above, choose the best value for k,   </span></span><br><span class="line"><span class="comment"># retrain the classifier using all the training data, and test it on the test</span></span><br><span class="line"><span class="comment"># data. You should be able to get above 26% accuracy on the test data.</span></span><br><span class="line"></span><br><span class="line">best_k = <span class="literal">None</span></span><br><span class="line"><span class="comment"># YOUR CODE HERE</span></span><br><span class="line"><span class="comment"># Choose the best k based on the cross validation above</span></span><br><span class="line">best_k =<span class="number">2</span></span><br><span class="line"><span class="comment"># END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">y_test_pred = predict_labels(dists, y_train, k=best_k)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute and display the accuracy</span></span><br><span class="line">num_correct = np.<span class="built_in">sum</span>(y_test_pred == y_test)</span><br><span class="line">accuracy = <span class="built_in">float</span>(num_correct) / num_test</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;For k = %d, got %d / %d correct =&gt; accuracy: %f&#x27;</span> % (best_k, num_correct, num_test, accuracy))</span><br></pre></td></tr></table></figure>

<pre><code>For k = 2, got 42 / 160 correct =&gt; accuracy: 0.262500
</code></pre>
<p>既然k&#x3D;2和k&#x3D;17准确率相同，应该是选择k较小的，这样可以提高运行速率吧。</p>
<hr>
<h2 id="Part-3-PCA-30-points"><a href="#Part-3-PCA-30-points" class="headerlink" title="Part 3: PCA (30 points)"></a>Part 3: PCA (30 points)</h2><p>Principal Component Analysis (PCA) is a simple yet popular and useful linear transformation technique that is used in numerous applications, such as stock market predictions, the analysis of gene expression data, and many more. In this tutorial, we will see that PCA is not just a “black box”, and we are going to unravel its internals in 3 basic steps.</p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>The sheer size of data in the modern age is not only a challenge for computer hardware but also a main bottleneck for the performance of many machine learning algorithms. The main goal of a PCA analysis is to identify patterns in data; PCA aims to detect the correlation between variables. If a strong correlation between variables exists, the attempt to reduce the dimensionality only makes sense. In a nutshell, this is what PCA is all about: Finding the directions of maximum variance in high-dimensional data and project it onto a smaller dimensional subspace while retaining most of the information.</p>
<h3 id="A-Summary-of-the-PCA-Approach"><a href="#A-Summary-of-the-PCA-Approach" class="headerlink" title="A Summary of the PCA Approach"></a>A Summary of the PCA Approach</h3><ul>
<li>Standardize the data.</li>
<li>Obtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix, or perform Singular Vector Decomposition.</li>
<li>Sort eigenvalues in descending order and choose the $k$ eigenvectors that correspond to the $k$ largest eigenvalues where $k$ is the number of dimensions of the new feature subspace ($k \leq d$).</li>
<li>Construct the projection matrix $\mathbf{W}$ from the selected $k$ eigenvectors.</li>
<li>Transform the original dataset $\mathbf{X}$ via $\mathbf{W}$ to obtain a $k$-dimensional feature subspace Y.</li>
</ul>
<blockquote>
<p><strong>博客推荐</strong></p>
<p><a target="_blank" rel="noopener" href="http://www.cnblogs.com/liu-jun/archive/2013/03/20/2970132.html">浅谈对主成分分析（PCA）算法的理解</a></p>
</blockquote>
<p>PCA方法（主成分分析）的总结</p>
<p>PCA的主旨就是：在高维数据中找到最大方差的方向，并将高维数据投射到低维的子空间，这样能够保留尽可能多的信息。</p>
<ul>
<li>标准化数据</li>
<li>从协方差矩阵或互相关矩阵或者用SVD奇异值分解，来获得特征向量和特征值</li>
<li>将特征值降序排序，根据新特征空间的维度k，选择前k个最大的特征值和对应的特征向量（k &lt;&#x3D; d）</li>
<li>根据k个特征向量构建投影矩阵W</li>
<li>将数据集X通过W转换到k维的特征子空间</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> features <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line">pca = PCA()</span><br></pre></td></tr></table></figure>

<h3 id="3-1-Eigendecomposition"><a href="#3-1-Eigendecomposition" class="headerlink" title="3.1 - Eigendecomposition"></a>3.1 - Eigendecomposition</h3><p>The eigenvectors and eigenvalues of a covariance (or correlation) matrix represent the “core” of a PCA: The eigenvectors (principal components) determine the directions of the new feature space, and the eigenvalues determine their magnitude. In other words, the eigenvalues explain the variance of the data along the new feature axes.</p>
<p>Implement <strong><code>_eigen_decomp</code></strong> in <code>pca.py</code>.</p>
<p>特征向量和特征值代表了PCA的核心：</p>
<ul>
<li>特征向量（主成分）决定了新的特征空间的方向</li>
<li>特征值决定了特征向量的幅度，换言之，特征值表示了数据在新特征方向的方差。方差越大，信息量越大。</li>
</ul>
<blockquote>
<p><strong>博客推荐</strong></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/Mr_HHH/article/details/78490576">协方差矩阵计算方法</a></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Perform eigenvalue decomposition on the covariance matrix of training data.</span></span><br><span class="line"><span class="comment"># 在训练数据的协方差矩阵上执行特征分解</span></span><br><span class="line">t1 = time()</span><br><span class="line">e_vecs, e_vals = pca._eigen_decomp(X_train - X_train.mean(axis=<span class="number">0</span>))</span><br><span class="line">t2 = time()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(e_vals.shape)</span><br><span class="line"><span class="built_in">print</span>(e_vecs.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;耗时：&quot;</span>, t2-t1)</span><br></pre></td></tr></table></figure>

<pre><code>(4096,)
(4096, 4096)
耗时： 32.20034837722778
</code></pre>
<p>直接解特征向量太慢了。</p>
<h3 id="3-2-Singular-Value-Decomposition"><a href="#3-2-Singular-Value-Decomposition" class="headerlink" title="3.2 - Singular Value Decomposition"></a>3.2 - Singular Value Decomposition</h3><p>Doing an eigendecomposition of the covariance matrix is very expensive, especially when the number of features (<code>D = 4096</code> here) gets very high.</p>
<p>To obtain the same eigenvalues and eigenvectors in a more efficient way, we can use Singular Value Decomposition (SVD). If we perform SVD on matrix $X$, we obtain $U$, $S$ and $V$ such that:<br>$$<br>X &#x3D; U S V^T<br>$$</p>
<ul>
<li>the columns of $U$ are the eigenvectors of $X X^T$</li>
<li>the columns of $V^T$ are the eigenvectors of $X^T X$</li>
<li>the values of $S$ are the square roots of the eigenvalues of $X^T X$ (or $X X^T$)</li>
</ul>
<p>Therefore, we can find out the top <code>k</code> eigenvectors of the covariance matrix $X^T X$ using SVD.</p>
<p>Implement <strong><code>_svd</code></strong> in <code>pca.py</code>.</p>
<p>这里讲到了直接利用协方差矩阵计算特征分解是非常耗时的事情，特别是维度很高的情况下。</p>
<p>其实更高效的方式是使用SVD（奇异值分解）。</p>
<p>具体的左奇异向量和右奇异向量与特征向量的对应关系在上面描述了，所以利用SVD就能得到特征向量和特征值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Perform SVD on directly on the training data.</span></span><br><span class="line">t1 = time()</span><br><span class="line">u, s = pca._svd(X_train - X_train.mean(axis=<span class="number">0</span>))</span><br><span class="line">t2 = time()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(s.shape)</span><br><span class="line"><span class="built_in">print</span>(u.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;耗时：&quot;</span>, t2-t1)</span><br></pre></td></tr></table></figure>

<pre><code>(800,)
(4096, 4096)
耗时： 1.3446221351623535
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Test whether the square of singular values and eigenvalues are the same.</span></span><br><span class="line"><span class="comment"># We also observe that `e_vecs` and `u` are the same (only the sign of each column can differ).</span></span><br><span class="line"><span class="comment"># 这里测试直接分解协方差矩阵和使用SVD方法得到的是否相同</span></span><br><span class="line">N = X_train.shape[<span class="number">0</span>]</span><br><span class="line"><span class="keyword">assert</span> np.allclose((s ** <span class="number">2</span>) / (N - <span class="number">1</span>), e_vals[:<span class="built_in">len</span>(s)])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s) - <span class="number">1</span>):</span><br><span class="line">    <span class="keyword">assert</span> np.allclose(e_vecs[:, i], u[:, i]) <span class="keyword">or</span> np.allclose(e_vecs[:, i], -u[:, i])</span><br><span class="line">    <span class="comment"># (the last eigenvector for i = len(s) - 1 is very noisy because the eigenvalue is almost 0,</span></span><br><span class="line">    <span class="comment">#  so imprecisions in the computation build up)</span></span><br></pre></td></tr></table></figure>

<h3 id="3-3-Dimensionality-Reduction"><a href="#3-3-Dimensionality-Reduction" class="headerlink" title="3.3 - Dimensionality Reduction"></a>3.3 - Dimensionality Reduction</h3><p>The top $k$ principal components explain most of the variance of the underlying data.</p>
<p>By projecting our initial data (the images) onto the subspace spanned by the top k principal components,<br>we can reduce the dimension of our inputs while keeping most of the information.</p>
<p>In the example below, we can see that <strong>using the first two components in PCA is not enough</strong> to allow us to see pattern in the data. All the classes seem placed at random in the 2D plane.</p>
<p><strong>维度缩减</strong></p>
<p>通过将前k个主成分投影在子空间，就可以达到缩减维度的目的，同时能够保留最大的信息。</p>
<p>需要在<code>feature.py</code>中实现 <code>transform</code> 函数，同时在<code>fit</code>函数中实现X的均值计算。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Dimensionality reduction by projecting the data onto</span></span><br><span class="line"><span class="comment"># lower dimensional subspace spanned by k principal components</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># To visualize, we will project in 2 dimensions</span></span><br><span class="line">n_components = <span class="number">2</span></span><br><span class="line">pca.fit(X_train)</span><br><span class="line">X_proj = pca.transform(X_train, n_components)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the top two principal components</span></span><br><span class="line"><span class="keyword">for</span> y <span class="keyword">in</span> np.unique(y_train):</span><br><span class="line">    plt.scatter(X_proj[y_train==y,<span class="number">0</span>], X_proj[y_train==y,<span class="number">1</span>], label=classes[y])</span><br><span class="line">    </span><br><span class="line">plt.xlabel(<span class="string">&#x27;1st component&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;2nd component&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img data-src="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511089/blog/wlxmgfikbtn6sp6mumdj.png" alt="png"></p>
<p><strong>一脸懵逼，这里我们可以看到，仅仅用两个最大的主成分是无法看出数据的特征来的。</strong></p>
<h3 id="3-4-Visualizing-Eigenfaces"><a href="#3-4-Visualizing-Eigenfaces" class="headerlink" title="3.4 - Visualizing Eigenfaces"></a>3.4 - Visualizing Eigenfaces</h3><p>The columns of the PCA projection matrix <code>pca.W_pca</code> represent the eigenvectors of $X^T X$.</p>
<p>We can visualize the biggest singular values as well as the corresponding vectors to get a sense of what the PCA algorithm is extracting.</p>
<p>If we visualize the top 10 eigenfaces, we can see tht the algorithm focuses on the different shades of the faces. For instance, in face n°2 the light seems to come from the left.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">10</span>, i+<span class="number">1</span>)</span><br><span class="line">    plt.imshow(pca.W_pca[:, i].reshape(<span class="number">64</span>, <span class="number">64</span>))</span><br><span class="line">    plt.title(<span class="string">&quot;%.2f&quot;</span> % s[i])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img data-src="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511090/blog/tjq8g5fgcrf2scve7mdg.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Reconstruct data with principal components</span></span><br><span class="line"><span class="comment"># 利用主成分进行数据重构，重构实际上就是投影的逆向操作。</span></span><br><span class="line">n_components = <span class="number">100</span>  <span class="comment"># 测试不同的参数值查看对重构的影响</span></span><br><span class="line">X_proj = pca.transform(X_train, n_components)</span><br><span class="line">X_rec = pca.reconstruct(X_proj)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(X_rec.shape)</span><br><span class="line"><span class="built_in">print</span>(classes)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看重构后的脸部图像</span></span><br><span class="line">samples_per_class = <span class="number">10</span></span><br><span class="line"><span class="keyword">for</span> y, cls <span class="keyword">in</span> <span class="built_in">enumerate</span>(classes):</span><br><span class="line">    idxs = np.flatnonzero(y_train == y)</span><br><span class="line">    idxs = np.random.choice(idxs, samples_per_class, replace=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">for</span> i, idx <span class="keyword">in</span> <span class="built_in">enumerate</span>(idxs):</span><br><span class="line">        plt_idx = i * num_classes + y + <span class="number">1</span></span><br><span class="line">        plt.subplot(samples_per_class, num_classes, plt_idx)</span><br><span class="line">        plt.imshow((X_rec[idx]).reshape((<span class="number">64</span>, <span class="number">64</span>)))</span><br><span class="line">        plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">            plt.title(y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>(800, 4096)
[&#39;angelina jolie&#39;, &#39;anne hathaway&#39;, &#39;barack obama&#39;, &#39;brad pitt&#39;, &#39;cristiano ronaldo&#39;, &#39;emma watson&#39;, &#39;george clooney&#39;, &#39;hillary clinton&#39;, &#39;jennifer aniston&#39;, &#39;johnny depp&#39;, &#39;justin timberlake&#39;, &#39;leonardo dicaprio&#39;, &#39;natalie portman&#39;, &#39;nicole kidman&#39;, &#39;scarlett johansson&#39;, &#39;tom cruise&#39;]
</code></pre>
<p><img data-src="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511090/blog/uyaie9xmwg228mq6mip1.png" alt="png"></p>
<p>哈哈，100个主成分能够分清男女了，200个主成分大概能够分清是哪个人了。</p>
<h3 id="Written-Question-1-5-points"><a href="#Written-Question-1-5-points" class="headerlink" title="Written Question 1 (5 points)"></a>Written Question 1 (5 points)</h3><p><em>Question</em>: Consider a dataset of $N$ face images, each with shape $(h, w)$. Then, we need $O(Nhw)$ of memory to store the data. Suppose we perform dimensionality reduction on the dataset with $p$ principal components, and use the components as bases to represent images. Calculate how much memory we need to store the images and the matrix used to get back to the original space.</p>
<p>Said in another way, how much memory does storing the compressed images <strong>and</strong> the uncompresser cost.</p>
<p><em>Answer:</em> TODO</p>
<p><strong>问题：压缩的图像占用的内存空间和解压器需要占用的内存空间</strong></p>
<p>答：</p>
<ol>
<li><p>降维后的图像维度为 <code>X_proj.shape = (N, n_components)</code>，所以实际压缩的图像占用空间即为$O(Np)$</p>
</li>
<li><p>为了实现压缩和复原的其他资源（姑且称之为解压器），需要保存的变量包括<code>self.W_pca</code>和<code>self.mean</code>，实际上内存大头就是样本的所有主成分表，所以解压器占用的内存空间为$O(pD)$ &#x3D; $O(phw)$</p>
</li>
</ol>
<h3 id="3-5-Reconstruction-error-and-captured-variance"><a href="#3-5-Reconstruction-error-and-captured-variance" class="headerlink" title="3.5 - Reconstruction error and captured variance"></a>3.5 - Reconstruction error and captured variance</h3><p>We can plot the reconstruction error with respect to the dimension of the projected space.</p>
<p>The reconstruction gets better with more components.</p>
<p>We can see in the plot that the inflexion point is around dimension 200 or 300. This means that using this number of components is a good compromise between good reconstruction and low dimension.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot reconstruction errors for different k</span></span><br><span class="line"><span class="comment"># 绘制出不同主成分数目对应的重构误差</span></span><br><span class="line">N = X_train.shape[<span class="number">0</span>]</span><br><span class="line">d = X_train.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">ns = <span class="built_in">range</span>(<span class="number">1</span>, d, <span class="number">20</span>)</span><br><span class="line">errors = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> ns:</span><br><span class="line">    X_proj = pca.transform(X_train, n)</span><br><span class="line">    X_rec = pca.reconstruct(X_proj)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute reconstruction error</span></span><br><span class="line">    error = np.mean((X_rec - X_train) ** <span class="number">2</span>)</span><br><span class="line">    errors.append(error)</span><br><span class="line"></span><br><span class="line">plt.plot(ns, errors)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Number of Components&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Reconstruction Error&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img data-src="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511091/blog/ucsqof75wk9yhwrruod7.png" alt="png"></p>
<p>从上面这幅图可以获取到的有用信息就是：</p>
<ol>
<li>1000以内，误差随着主成分数目增加而减小</li>
<li>大概在200-400间，出现了下降速率的拐角，此处可以来个权衡，因为主成分数目并不是越多越好，它影响了运行的效率。选择拐角处的值，既能保证误差率较低，还能兼顾计算时间。</li>
</ol>
<p>We can do the same process to see how much variance is captured by the projection.</p>
<p>Again, we see that the inflexion point is around 200 or 300 dimensions.</p>
<p>同样，方差的拐点也在200-400之间</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Plot captured variance</span></span><br><span class="line">ns = <span class="built_in">range</span>(<span class="number">1</span>, d, <span class="number">100</span>)</span><br><span class="line">var_cap = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> ns:</span><br><span class="line">    var_cap.append(np.<span class="built_in">sum</span>(s[:n] ** <span class="number">2</span>)/np.<span class="built_in">sum</span>(s ** <span class="number">2</span>))</span><br><span class="line">    </span><br><span class="line">plt.plot(ns, var_cap)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Number of Components&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Variance Captured&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img data-src="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511091/blog/cjq1hg59vcc5ik9owzsd.png" alt="png"></p>
<h3 id="3-6-kNN-with-PCA"><a href="#3-6-kNN-with-PCA" class="headerlink" title="3.6 - kNN with PCA"></a>3.6 - kNN with PCA</h3><p>Performing kNN on raw features (the pixels of the image) does not yield very good results.<br>Computing the distance between images in the image space is not a very good metric for actual proximity of images.<br>For instance, an image of person A with a dark background will be close to an image of B with a dark background, although these people are not the same.</p>
<p>Using a technique like PCA can help discover the real interesting features and perform kNN on them could give better accuracy.</p>
<p>However, we observe here that PCA doesn’t really help to disentangle the features and obtain useful distance metrics between the different classes. We basically obtain the same performance as with raw features.</p>
<p>上面这段话讲的挺好，直接使用原始的特征（即图像的像素）并不能表现的很好，因为在计算图像之间的距离时，两个不同物体具有相同的黑色背景时，背景明显在计算上减小了两者之间的距离，虽然我们要判断的是前景的类别。</p>
<p>使用PCA能够得到有趣的特征，能够使kNN能够达到更高的准确率。</p>
<p>但是，在这里，PCA并不能在不同类别里区分开特征，获取到有用的距离。这里看起来和使用像素是一样的效果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">num_test = X_test.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># We computed the best k and n for you</span></span><br><span class="line">best_k = <span class="number">20</span></span><br><span class="line">best_n = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># PCA</span></span><br><span class="line">pca = PCA()</span><br><span class="line">pca.fit(X_train)</span><br><span class="line">X_proj = pca.transform(X_train, best_n)</span><br><span class="line">X_test_proj = pca.transform(X_test, best_n)</span><br><span class="line"></span><br><span class="line"><span class="comment"># kNN</span></span><br><span class="line">dists = compute_distances(X_test_proj, X_proj)</span><br><span class="line">y_test_pred = predict_labels(dists, y_train, k=best_k)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute and display the accuracy</span></span><br><span class="line">num_correct = np.<span class="built_in">sum</span>(y_test_pred == y_test)</span><br><span class="line">accuracy = <span class="built_in">float</span>(num_correct) / num_test</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Got %d / %d correct =&gt; accuracy: %f&#x27;</span> % (num_correct, num_test, accuracy))</span><br></pre></td></tr></table></figure>

<pre><code>Got 45 / 160 correct =&gt; accuracy: 0.281250
</code></pre>
<p>呵呵哒，我知道PCA有用，但就涨了这么点准确率，我怎么能信服。</p>
<hr>
<h2 id="Part-4-Fisherface-Linear-Discriminant-Analysis-25-points"><a href="#Part-4-Fisherface-Linear-Discriminant-Analysis-25-points" class="headerlink" title="Part 4 - Fisherface: Linear Discriminant Analysis (25 points)"></a>Part 4 - Fisherface: Linear Discriminant Analysis (25 points)</h2><p>LDA is a linear transformation method like PCA, but with a different goal.<br>The main difference is that LDA takes information from the labels of the examples to maximize the separation of the different classes in the transformed space.</p>
<p>Therefore, LDA is not totally <strong>unsupervised</strong> since it requires labels. PCA is <strong>fully unsupervised</strong>.</p>
<p>In summary:</p>
<ul>
<li>PCA perserves maximum variance in the projected space.</li>
<li>LDA preserves discrimination between classes in the project space. We want to maximize scatter between classes and minimize scatter intra class.</li>
</ul>
<blockquote>
<p><strong>博客推荐</strong></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html">机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)</a></p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/34305879">如何理解LDA算法？能够简洁明了地说明一下LDA算法的中心思想吗？</a></p>
</blockquote>
<p><strong>LDA和PCA的区别</strong></p>
<p>LDA的主旨就是从标签中获取信息，在转换的目标空间最大化不同种类之间的隔离。</p>
<p>盗了一张图作说明，<br><img data-src="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511092/blog/wwwgwlp65dxempx2kflv.jpg" alt="LDA表示"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> features <span class="keyword">import</span> LDA</span><br><span class="line"></span><br><span class="line">lda = LDA()</span><br></pre></td></tr></table></figure>

<h3 id="4-1-Dimensionality-Reduction-via-PCA"><a href="#4-1-Dimensionality-Reduction-via-PCA" class="headerlink" title="4.1 - Dimensionality Reduction via PCA"></a>4.1 - Dimensionality Reduction via PCA</h3><p>To apply LDA, we need $D &lt; N$. Since in our dataset, $N &#x3D; 800$ and $D &#x3D; 4096$, we first need to reduce the number of dimensions of the images using PCA.<br>More information at: <a target="_blank" rel="noopener" href="http://www.scholarpedia.org/article/Fisherfaces">http://www.scholarpedia.org/article/Fisherfaces</a></p>
<p><strong>通过PCA实现维度的缩减</strong></p>
<p>这里提到数据特征维度要比样本数少，所以需要进行一步特征的维度缩减，就直接拿PCA做降维好了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">N = X_train.shape[<span class="number">0</span>]</span><br><span class="line">c = num_classes</span><br><span class="line"></span><br><span class="line">pca = PCA()</span><br><span class="line">pca.fit(X_train)</span><br><span class="line">X_train_pca = pca.transform(X_train, N-c)</span><br><span class="line">X_test_pca = pca.transform(X_test, N-c)</span><br></pre></td></tr></table></figure>

<h3 id="4-2-Scatter-matrices"><a href="#4-2-Scatter-matrices" class="headerlink" title="4.2 - Scatter matrices"></a>4.2 - Scatter matrices</h3><p>We first need to compute the within-class scatter matrix:<br>$$<br>S_W &#x3D; \sum_{i&#x3D;1}^c S_i<br>$$<br>where $S_i &#x3D; \sum_{x_k \in Y_i} (x_k - \mu_i)(x_k - \mu_i)^T$ is the scatter of class $i$.</p>
<p>We then need to compute the between-class scatter matrix:<br>$$<br>S_B &#x3D; \sum_{i&#x3D;1}^c N_i (\mu_i - \mu)(\mu_i - \mu)^T<br>$$<br>where $N_i$ is the number of examples in class $i$.</p>
<p><strong>头疼的数学：散点图矩阵</strong></p>
<p>上面的公式偏向结论的，具体的推导在Lecture13 的PPT 第42页开始。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Compute within-class scatter matrix</span></span><br><span class="line"><span class="comment"># 计算类内散点图矩阵</span></span><br><span class="line">S_W = lda._within_class_scatter(X_train_pca, y_train)</span><br><span class="line"><span class="built_in">print</span>(S_W.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(784, 784)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Compute between-class scatter matrix</span></span><br><span class="line"><span class="comment"># 计算类间散点图矩阵</span></span><br><span class="line">S_B = lda._between_class_scatter(X_train_pca, y_train)</span><br><span class="line"><span class="built_in">print</span>(S_B.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(784, 784)
</code></pre>
<h3 id="4-3-Solving-generalized-Eigenvalue-problem"><a href="#4-3-Solving-generalized-Eigenvalue-problem" class="headerlink" title="4.3 - Solving generalized Eigenvalue problem"></a>4.3 - Solving generalized Eigenvalue problem</h3><p>Implement methods <code>fit</code> and <code>transform</code> of the <code>LDA</code> class.</p>
<p>哈哈，根据课堂PPT第55页，最终将优化问题泛化为了一个求特征值的问题。编程时直接使用线性代数库求解特征值即可。</p>
<p><img data-src="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511092/blog/fjkefvwgvpny1i24r6qk.png" alt="推导一"><br><img data-src="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511093/blog/qxiyfyuferuubuvlioog.png" alt="推导二"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lda.fit(X_train_pca, y_train)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Dimensionality reduction by projecting the data onto</span></span><br><span class="line"><span class="comment"># lower dimensional subspace spanned by k principal components</span></span><br><span class="line"><span class="comment"># 通过K个主成分，将数据投射到低维的子空间上来实现维度缩减</span></span><br><span class="line"><span class="comment"># 这个实现方式其实跟PCA应该是一样的</span></span><br><span class="line">n_components = <span class="number">2</span></span><br><span class="line">X_proj = lda.transform(X_train_pca, n_components)</span><br><span class="line"></span><br><span class="line">X_test_proj = lda.transform(X_test_pca, n_components)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the top two principal components on the training set</span></span><br><span class="line"><span class="keyword">for</span> y <span class="keyword">in</span> np.unique(y_train):</span><br><span class="line">    plt.scatter(X_proj[y_train==y, <span class="number">0</span>], X_proj[y_train==y, <span class="number">1</span>], label=classes[y])</span><br><span class="line">    </span><br><span class="line">plt.xlabel(<span class="string">&#x27;1st component&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;2nd component&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(<span class="string">&quot;Training set&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the top two principal components on the test set</span></span><br><span class="line"><span class="keyword">for</span> y <span class="keyword">in</span> np.unique(y_test):</span><br><span class="line">    plt.scatter(X_test_proj[y_test==y, <span class="number">0</span>], X_test_proj[y_test==y,<span class="number">1</span>], label=classes[y])</span><br><span class="line">    </span><br><span class="line">plt.xlabel(<span class="string">&#x27;1st component&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;2nd component&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(<span class="string">&quot;Test set&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img data-src="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511094/blog/vepeiprwymewx0lk7tmx.png" alt="png"></p>
<p><img data-src="https://res.cloudinary.com/dgchmgebr/image/upload/v1678511094/blog/tnfpo9wwgt8hqivsnal1.png" alt="png"></p>
<p>第一幅图的效果好是因为已经fit过了，即进行过了训练。</p>
<p>第二幅图的效果不好，泛化能力明显不行。</p>
<p><strong>实现维度缩减的最后一步，投射到子空间的方式和PCA是一样的，</strong></p>
<p><strong>那么PCA和LDA唯一的不同就是寻找主成分的理念不同了，</strong></p>
<p><strong>PCA强调的是在实现维度缩减的同时最大化保留的信息量</strong></p>
<p><strong>LDA强调的则是最小化类内的区别，最大化类间的区别</strong></p>
<h3 id="4-4-kNN-with-LDA"><a href="#4-4-kNN-with-LDA" class="headerlink" title="4.4 - kNN with LDA"></a>4.4 - kNN with LDA</h3><p>Thanks to having the information from the labels, LDA gives a discriminant space where the classes are far apart from each other.<br>This should help kNN a lot, as the job should just be to find the obvious 10 clusters.</p>
<p>However, as we’ve seen in the previous plot (section 4.3), the training data gets clustered pretty well, but the test data isn’t as nicely clustered as the training data (overfitting?).</p>
<p>Perform cross validation following the code below (you can change the values of <code>k_choices</code> and <code>n_choices</code> to search). Using the best result from cross validation, obtain the test accuracy.</p>
<p><strong>LDA具有最大化类间差距最小化类内差距的功效，所以在分类问题能够有很大帮助</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">num_folds = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">X_trains, y_trains, X_vals, y_vals = split_folds(X_train, y_train, num_folds)</span><br><span class="line"></span><br><span class="line">k_choices = [<span class="number">1</span>, <span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>]</span><br><span class="line">n_choices = [<span class="number">5</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">50</span>, <span class="number">100</span>, <span class="number">200</span>, <span class="number">500</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># k_choices = [5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22]</span></span><br><span class="line"><span class="comment"># n_choices = [10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># n_k_to_accuracies[(n, k)] should be a list of length num_folds giving the different</span></span><br><span class="line"><span class="comment"># accuracy values that we found when using that value of n and k.</span></span><br><span class="line">n_k_to_accuracies = defaultdict(<span class="built_in">list</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_folds):</span><br><span class="line">    <span class="comment"># Fit PCA</span></span><br><span class="line">    pca = PCA()</span><br><span class="line">    pca.fit(X_trains[i])</span><br><span class="line">    </span><br><span class="line">    N = <span class="built_in">len</span>(X_trains[i])</span><br><span class="line">    X_train_pca = pca.transform(X_trains[i], N-c)</span><br><span class="line">    X_val_pca = pca.transform(X_vals[i], N-c)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Fit LDA</span></span><br><span class="line">    lda = LDA()</span><br><span class="line">    lda.fit(X_train_pca, y_trains[i])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> n <span class="keyword">in</span> n_choices:</span><br><span class="line">        X_train_proj = lda.transform(X_train_pca, n)</span><br><span class="line">        X_val_proj = lda.transform(X_val_pca, n)</span><br><span class="line"></span><br><span class="line">        dists = compute_distances(X_val_proj, X_train_proj)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> k_choices:</span><br><span class="line">            y_pred = predict_labels(dists, y_trains[i], k=k)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Compute and print the fraction of correctly predicted examples</span></span><br><span class="line">            num_correct = np.<span class="built_in">sum</span>(y_pred == y_vals[i])</span><br><span class="line">            accuracy = <span class="built_in">float</span>(num_correct) / <span class="built_in">len</span>(y_vals[i])</span><br><span class="line">            n_k_to_accuracies[(n, k)].append(accuracy)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> n_choices:</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> k_choices:</span><br><span class="line">        accuracies = n_k_to_accuracies[(n, k)]</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;For n=%d, k=%d: average accuracy is %f&quot;</span> % (n, k, np.mean(accuracies)))</span><br></pre></td></tr></table></figure>

<pre><code>For n=5, k=1: average accuracy is 0.150000
For n=5, k=5: average accuracy is 0.142500
For n=5, k=10: average accuracy is 0.146250
For n=5, k=20: average accuracy is 0.140000

For n=10, k=1: average accuracy is 0.153750
For n=10, k=5: average accuracy is 0.152500
For n=10, k=10: average accuracy is 0.153750
For n=10, k=20: average accuracy is 0.151250

For n=20, k=1: average accuracy is 0.172500
For n=20, k=5: average accuracy is 0.180000
For n=20, k=10: average accuracy is 0.176250
For n=20, k=20: average accuracy is 0.191250

For n=50, k=1: average accuracy is 0.161250
For n=50, k=5: average accuracy is 0.181250
For n=50, k=10: average accuracy is 0.183750
For n=50, k=20: average accuracy is 0.172500

For n=100, k=1: average accuracy is 0.160000
For n=100, k=5: average accuracy is 0.172500
For n=100, k=10: average accuracy is 0.165000
For n=100, k=20: average accuracy is 0.150000

For n=200, k=1: average accuracy is 0.166250
For n=200, k=5: average accuracy is 0.180000
For n=200, k=10: average accuracy is 0.183750
For n=200, k=20: average accuracy is 0.168750

For n=500, k=1: average accuracy is 0.167500
For n=500, k=5: average accuracy is 0.162500
For n=500, k=10: average accuracy is 0.180000
For n=500, k=20: average accuracy is 0.181250
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Based on the cross-validation results above, choose the best value for k,   </span></span><br><span class="line"><span class="comment"># retrain the classifier using all the training data, and test it on the test</span></span><br><span class="line"><span class="comment"># data. You should be able to get above 40% accuracy on the test data.</span></span><br><span class="line"></span><br><span class="line">best_k = <span class="literal">None</span></span><br><span class="line">best_n = <span class="literal">None</span></span><br><span class="line"><span class="comment"># YOUR CODE HERE</span></span><br><span class="line"><span class="comment"># Choose the best k based on the cross validation above</span></span><br><span class="line">best_k =<span class="number">20</span></span><br><span class="line">best_n = <span class="number">500</span></span><br><span class="line"><span class="comment"># END YOUR CODE</span></span><br><span class="line"></span><br><span class="line">N = <span class="built_in">len</span>(X_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit PCA</span></span><br><span class="line">pca = PCA()</span><br><span class="line">pca.fit(X_train)</span><br><span class="line">X_train_pca = pca.transform(X_train, N-c)</span><br><span class="line">X_test_pca = pca.transform(X_test, N-c)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit LDA</span></span><br><span class="line">lda = LDA()</span><br><span class="line">lda.fit(X_train_pca, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Project using LDA</span></span><br><span class="line">X_train_proj = lda.transform(X_train_pca, best_n)</span><br><span class="line">X_test_proj = lda.transform(X_test_pca, best_n)</span><br><span class="line"></span><br><span class="line">dists = compute_distances(X_test_proj, X_train_proj)</span><br><span class="line">y_test_pred = predict_labels(dists, y_train, k=best_k)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Compute and display the accuracy</span></span><br><span class="line">num_correct = np.<span class="built_in">sum</span>(y_test_pred == y_test)</span><br><span class="line">accuracy = <span class="built_in">float</span>(num_correct) / num_test</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;For k=%d and n=%d&quot;</span> % (best_k, best_n))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Got %d / %d correct =&gt; accuracy: %f&#x27;</span> % (num_correct, num_test, accuracy))</span><br></pre></td></tr></table></figure>

<pre><code>For k=20 and n=500
Got 36 / 160 correct =&gt; accuracy: 0.225000
</code></pre>
<p><strong>一脸懵逼。说好的准确率的提升没有上去。感觉过程中没有问题啊</strong><br>我估计这次训练得到的LDA泛化能力不行，就跟上面那个测试一样。</p>
<h1 id="代码档案"><a href="#代码档案" class="headerlink" title="代码档案"></a>代码档案</h1><p><a target="_blank" rel="noopener" href="https://github.com/StanfordVL/CS131_release/tree/master/hw6_release">官方Repo作业材料</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/veraposeidon/CS131_Assignments/tree/master/hw6_release">个人Repo作业存档</a></p>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>xiaohai
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://shenxiaohai.me/cs131-homework6/" title="CS131,Homewrok6,Recognition-Classification">https://shenxiaohai.me/cs131-homework6/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" rel="tag"><i class="fa fa-tag"></i> 计算机视觉</a>
              <a href="/tags/CS131/" rel="tag"><i class="fa fa-tag"></i> CS131</a>
              <a href="/tags/PCA/" rel="tag"><i class="fa fa-tag"></i> PCA</a>
              <a href="/tags/LDA/" rel="tag"><i class="fa fa-tag"></i> LDA</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/cs131-homework5/" rel="prev" title="CS131,Homewrok5,Segmentation-Clustering">
                  <i class="fa fa-chevron-left"></i> CS131,Homewrok5,Segmentation-Clustering
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/cs131-homework7/" rel="next" title="CS131,Homewrok7,Object-Detection">
                  CS131,Homewrok7,Object-Detection <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments"><div id="twikoo-comments"></div></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2018 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-laptop"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">xiaohai</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/veraposeidon" class="github-corner" title="在 GitHub 上关注我" aria-label="在 GitHub 上关注我" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.8/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/next-boot.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>


  <script src="https://cdnjs.cloudflare.com/ajax/libs/quicklink/2.3.0/quicklink.umd.js" integrity="sha256-yvJQOINiH9fWemHn0vCA5lsHWJaHs6/ZmO+1Ft04SvM=" crossorigin="anonymous"></script>
  <script class="next-config" data-name="quicklink" type="application/json">{"enable":true,"home":true,"archive":true,"delay":true,"timeout":3000,"priority":true,"url":"https://shenxiaohai.me/cs131-homework6/"}</script>
  <script src="/js/third-party/quicklink.js"></script>
<script class="next-config" data-name="twikoo" type="application/json">{"enable":true,"visitor":false,"envId":"https://twikoo-comment.shenxiaohai.me/","el":"#twikoo-comments"}</script>
<script>
document.addEventListener('page:loaded', () => {
  NexT.utils.loadComments(CONFIG.twikoo.el)
    .then(() => NexT.utils.getScript(
      CONFIG.twikoo.jsUrl || 'https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js',
      { condition: window.twikoo }
    ))
    .then(() => {
      twikoo.init(CONFIG.twikoo);
    });
});
</script>
<style>
.post-block, .comments {
  overflow: visible;
}
.tk-owo-emotion {
  display: inline-block;
}
</style>

</body>
</html>
